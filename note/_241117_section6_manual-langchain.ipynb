{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1182, which is longer than the specified 600\n",
      "Created a chunk of size 1491, which is longer than the specified 600\n",
      "Created a chunk of size 1401, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1326, which is longer than the specified 600\n",
      "Created a chunk of size 1449, which is longer than the specified 600\n",
      "Created a chunk of size 1364, which is longer than the specified 600\n",
      "Created a chunk of size 999, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Victory Mansions is a building where Winston Smith resides. It is described as having glass doors through which gritty dust can enter. The hallway smells of boiled cabbage and old rag mats. There is a large colored poster on the wall depicting the face of a man with a black mustache. The building has a non-functioning lift due to the electricity being cut off during daylight hours as part of an economy drive. The flat where Winston lives is on the seventh floor, and there is a poster with a large face on each landing that seems to follow you as you move.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag(retrieval augmented generation): 검색 증강 기법\n",
    "# 아래는 stuff 방식을 manual하게 접근한 것입니다. \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"../.cache/\")\n",
    "\n",
    "\n",
    "# # chunk_size는 토큰의 길이를 제한 / chunk_overlap은 앞 부분 일부분을 겹치게 만들게 합니다.\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=100,\n",
    "#     chunk_overlap=10\n",
    "# )\n",
    "\n",
    "# 아래는 separator가 있습니다. \n",
    "# RecursiveCharacterTextSplitter의 기능을 모두 갖고 있으면서 seperator가 있기 때문에 해당 방법을 추천합니다. \n",
    "# 토큰화 방법에 대한 직관적인 방법을 보고 싶으면 해당 경로를 참고하세요\n",
    "# https://platform.openai.com/tokenizer\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap= 50,\n",
    ")\n",
    "\n",
    "# 해당 loader는 pdf, txt, docx와 모두 호환됩니다.\n",
    "loader = UnstructuredFileLoader(\"../files/_241117_chapter_one.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embedding에 대한 직관적인 이해가 필요하다면 다음 링크를 참고한다.\n",
    "# https://turbomaze.github.io/word2vecjson/\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "# FAISS와 CHROMA를 변경할 수 있다\n",
    "# vectorstore = Chroma.from_documents(docs[:10], cached_embeddings)\n",
    "vectorstore = FAISS.from_documents(docs[:10], cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriver, \n",
    "        \"question\": RunnablePassthrough(), \n",
    "    }\n",
    "    | prompt\n",
    "    | llm )\n",
    "\n",
    "chain.invoke(\"Desribe Victory Mansions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 963, which is longer than the specified 600\n",
      "Created a chunk of size 774, which is longer than the specified 600\n",
      "Created a chunk of size 954, which is longer than the specified 600\n",
      "Created a chunk of size 922, which is longer than the specified 600\n",
      "Created a chunk of size 1168, which is longer than the specified 600\n",
      "Created a chunk of size 821, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 745, which is longer than the specified 600\n",
      "Created a chunk of size 735, which is longer than the specified 600\n",
      "Created a chunk of size 1110, which is longer than the specified 600\n",
      "Created a chunk of size 991, which is longer than the specified 600\n",
      "Created a chunk of size 990, which is longer than the specified 600\n",
      "Created a chunk of size 1182, which is longer than the specified 600\n",
      "Created a chunk of size 1491, which is longer than the specified 600\n",
      "Created a chunk of size 1401, which is longer than the specified 600\n",
      "Created a chunk of size 1130, which is longer than the specified 600\n",
      "Created a chunk of size 1326, which is longer than the specified 600\n",
      "Created a chunk of size 1449, which is longer than the specified 600\n",
      "Created a chunk of size 1364, which is longer than the specified 600\n",
      "Created a chunk of size 999, which is longer than the specified 600\n",
      "Created a chunk of size 930, which is longer than the specified 600\n",
      "Created a chunk of size 1022, which is longer than the specified 600\n",
      "Created a chunk of size 1260, which is longer than the specified 600\n",
      "Created a chunk of size 795, which is longer than the specified 600\n",
      "Created a chunk of size 1293, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Winston goes to work at the Ministry of Truth.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rag(retrieval augmented generation): 검색 증강 기법\n",
    "# 아래는 map_reduce 방식을 manual하게 접근한 것입니다. \n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader, UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.1)\n",
    "\n",
    "cache_dir = LocalFileStore(\"../.cache/\")\n",
    "\n",
    "\n",
    "# # chunk_size는 토큰의 길이를 제한 / chunk_overlap은 앞 부분 일부분을 겹치게 만들게 합니다.\n",
    "# splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=100,\n",
    "#     chunk_overlap=10\n",
    "# )\n",
    "\n",
    "# 아래는 separator가 있습니다. \n",
    "# RecursiveCharacterTextSplitter의 기능을 모두 갖고 있으면서 seperator가 있기 때문에 해당 방법을 추천합니다. \n",
    "# 토큰화 방법에 대한 직관적인 방법을 보고 싶으면 해당 경로를 참고하세요\n",
    "# https://platform.openai.com/tokenizer\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator = '\\n',\n",
    "    chunk_size = 600,\n",
    "    chunk_overlap= 50,\n",
    ")\n",
    "\n",
    "# 해당 loader는 pdf, txt, docx와 모두 호환됩니다.\n",
    "loader = UnstructuredFileLoader(\"../files/_241117_chapter_one.txt\")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# embedding에 대한 직관적인 이해가 필요하다면 다음 링크를 참고한다.\n",
    "# https://turbomaze.github.io/word2vecjson/\n",
    "embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embeddings, cache_dir\n",
    ")\n",
    "# FAISS와 CHROMA를 변경할 수 있다\n",
    "# vectorstore = Chroma.from_documents(docs[:10], cached_embeddings)\n",
    "vectorstore = FAISS.from_documents(docs[:10], cached_embeddings)\n",
    "\n",
    "retriver = vectorstore.as_retriever()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "# list of docs\n",
    "\n",
    "# for doc in list of docs | prompt | llm\n",
    "\n",
    "# for response in list of llms response | put them all together\n",
    "\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            use the following portion of a long document to see if any of \n",
    "            the text is relevant to answer the question. Return any relevant text\n",
    "            verbatim\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\",\"{question}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "\n",
    "def map_docs(inputs):\n",
    "    documents = inputs['documents']\n",
    "    question = inputs['question']\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            map_doc_chain.invoke(\n",
    "                {\n",
    "                    \"context\":document.page_content,\n",
    "                    \"question\":question\n",
    "                }\n",
    "            ).content\n",
    "                for document in documents\n",
    "        ]\n",
    "    )\n",
    "\n",
    "map_chain = {\"documents\":retriver, 'question':RunnablePassthrough()} | RunnableLambda(map_docs)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given th following extracted parts of a long document and a question, \n",
    "            create a final answer.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\"\n",
    "        ),\n",
    "        (\"human\",\"{question}\"),\n",
    "    ]   \n",
    ")\n",
    "\n",
    "chain =(\n",
    "    {\n",
    "        \"context\": map_chain, \n",
    "        \"question\": RunnablePassthrough() \n",
    "    } \n",
    "    | final_prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "chain.invoke(\"Where does Winston go to work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
